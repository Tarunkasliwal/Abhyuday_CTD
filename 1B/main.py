import os
import json
import argparse
from datetime import datetime
from typing import List, Dict, Any
from collections import Counter
import re
import pdfplumber

# ==============================================================================
# All necessary libraries are included.
# ==============================================================================
import spacy
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# --- AI Models (loaded once for efficiency) ---
print("üß† Loading NLP models...")
try:
    nlp = spacy.load("en_core_web_sm")
    embedding_model = SentenceTransformer("sentence-transformers/paraphrase-MiniLM-L6-v2")
except Exception as e:
    print(f"‚ùå Error loading models: {e}")
    print("Please ensure you have run 'pip install -r requirements.txt' and 'python -m spacy download en_core_web_sm'")
    exit()
print("‚úÖ Models loaded successfully.")


# ==============================================================================
# ‚ú® NEW STRATEGY: CHUNK-AND-RANK ‚ú®
# ==============================================================================

def create_text_chunks(pdf_path: str, chunk_size: int = 5) -> List[Dict[str, Any]]:
    """
    Extracts text from a PDF and breaks it into smaller, overlapping chunks of sentences.
    """
    chunks = []
    with pdfplumber.open(pdf_path) as pdf:
        full_text = ""
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                full_text += page_text + "\n"
        
        doc = nlp(full_text)
        sents = list(doc.sents)
        for i in range(0, len(sents), chunk_size - 1): # Overlapping chunks
            chunk_sents = sents[i : i + chunk_size]
            if not chunk_sents: continue
            
            chunk_text = " ".join(sent.text for sent in chunk_sents).strip()
            # Estimate page number. This is an approximation.
            start_char_offset = chunk_sents[0].start_char
            page_num = 1 # Fallback
            # This logic for page number is complex; we'll simplify for the task
            
            chunks.append({
                "text": chunk_text,
                "source_doc": os.path.basename(pdf_path),
                "page": page_num # Simplified page attribution
            })
    return chunks

def find_heading_for_chunk(chunk: Dict[str, Any]) -> str:
    """
    For a given chunk, opens its source PDF and finds the nearest preceding heading.
    This is a simplified heuristic for the final output.
    """
    # This is a complex task. For the hackathon, a simpler approach is better.
    # We will refine the main pipeline to make this step unnecessary by changing
    # how we generate the final output.
    # For now, we will focus on getting the *right content* first.
    # Let's return a placeholder that we will refine later.
    return "Relevant Section" # Placeholder

# --- Main Ranking and Generation Logic ---

def run_intelligence_pipeline(pdf_dir: str, persona: str, job_to_be_done: str, output_file: str):
    print("üöÄ Starting Persona-Driven Document Intelligence Pipeline with Chunk-and-Rank...")
    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith(".pdf")]
    if not pdf_files: print(f"‚ùå No PDF files found in directory: {pdf_dir}"); return

    # --- STEP 1: CHUNK ---
    print(f"üîé Found {len(pdf_files)} PDFs. Breaking all documents into text chunks...")
    all_chunks = []
    for pdf_file in pdf_files:
        print(f"  -> Chunking document: {pdf_file}")
        pdf_path = os.path.join(pdf_dir, pdf_file)
        all_chunks.extend(create_text_chunks(pdf_path))
    
    if not all_chunks: print("‚ùå No text could be extracted."); return
    print(f"üìÑ Created {len(all_chunks)} total text chunks.")

    # --- STEP 2: RANK ---
    query = f"{persona}. {job_to_be_done}"
    print("üß† Ranking all chunks against the query to find the most relevant content...")
    chunk_texts = [chunk["text"] for chunk in all_chunks]
    chunk_embeddings = get_text_embeddings(chunk_texts)
    query_embedding = get_text_embeddings([query])
    
    sim_scores = cosine_similarity(query_embedding, chunk_embeddings)[0]
    
    for i, chunk in enumerate(all_chunks):
        chunk["similarity"] = sim_scores[i]
        
    ranked_chunks = sorted(all_chunks, key=lambda x: x["similarity"], reverse=True)
    
    # --- STEP 3: ATTRIBUTE & GENERATE OUTPUT ---
    print("‚ú® Generating final output from top-ranked content...")
    
    # For sub_section_analysis, use the top 5-10 chunks directly
    top_snippets = ranked_chunks[:10]
    subsection_analysis = [
        {
            "document": s["source_doc"],
            "refined_text": s["text"],
            "page_number": s["page"] # Note: This is an approximation
        } for s in top_snippets
    ]

    # For extracted_sections, we need to be clever. Let's find the best sections
    # that *contain* our top snippets.
    # We will use a simplified parser just for this attribution step.
    
    # Simplified Heading Finder
    def get_heading_for_text(pdf_path, text_snippet):
        with pdfplumber.open(pdf_path) as pdf:
            full_text = "".join(p.extract_text() or "" for p in pdf.pages)
            if text_snippet not in full_text:
                return "General Information" # Snippet not found

            # Find the most common font style (body text)
            font_styles = Counter((round(c.get("size", 0)), c.get("fontname")) for p in pdf.pages for c in p.chars)
            if not font_styles: return "General Information"
            body_size, body_font = font_styles.most_common(1)[0][0]

            # Find the heading just before the snippet
            last_heading = os.path.basename(pdf_path).replace('.pdf', '')
            for page in pdf.pages:
                lines = page.extract_words(x_tolerance=3, y_tolerance=3, keep_blank_chars=False, use_text_flow=True)
                page_lines = {}
                for word in lines:
                    y0 = round(word['top'])
                    if y0 not in page_lines: page_lines[y0] = []
                    page_lines[y0].append(word)

                for y0 in sorted(page_lines.keys()):
                    line_words = sorted(page_lines[y0], key=lambda w: w['x0'])
                    line_text = " ".join(word['text'] for word in line_words).strip()
                    if not line_text: continue
                    
                    if text_snippet in current_text: # Stop when we pass the snippet
                        return last_heading

                    first_word = line_words[0]
                    if round(first_word.get('size',0)) > body_size or first_word.get('fontname') != body_font:
                        last_heading = line_text

        return last_heading

    extracted_sections = []
    seen_titles = set()
    for snippet in top_snippets:
        if len(extracted_sections) >= 5: break
        
        title = get_heading_for_text(os.path.join(pdf_dir, snippet["source_doc"]), snippet["text"])
        
        unique_key = (snippet["source_doc"], title)
        if title and unique_key not in seen_titles:
            extracted_sections.append({
                "document": snippet["source_doc"],
                "section_title": title,
                "importance_rank": len(extracted_sections) + 1,
                "page_number": snippet["page"]
            })
            seen_titles.add(unique_key)

    metadata = {"input_documents": pdf_files, "persona": persona, "job_to_be_done": job_to_be_done, "processing_timestamp": datetime.utcnow().isoformat() + "Z"}
    
    # Final JSON generation
    output_data = {"metadata": metadata, "extracted_sections": extracted_sections, "sub_section_analysis": subsection_analysis[:5]}
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=4, ensure_ascii=False)
    print(f"‚úÖ Successfully generated output at: {output_file}")


# --- Other functions remain unchanged ---
def get_text_embeddings(texts: List[str]) -> np.ndarray:
    return embedding_model.encode(texts, convert_to_tensor=False)


# --- Interactive Execution Block ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Compliant Document Intelligence Engine with Chunk-and-Rank.")
    parser.add_argument("--pdf_dir", type=str, help="Directory containing input PDF files.")
    parser.add_argument("--persona", type=str, help="The user persona.")
    parser.add_argument("--job", type=str, help="The job-to-be-done for the persona.")
    parser.add_argument("--output_file", type=str, default="output.json", help="Path to save the output JSON file.")
    args = parser.parse_args()

    pdf_dir = args.pdf_dir if args.pdf_dir else input("‚û°Ô∏è Enter the path to your PDF directory: ")
    persona = args.persona if args.persona else input("‚û°Ô∏è Who is the user? (e.g., HR professional): ")
    job_to_be_done = args.job if args.job else input("‚û°Ô∏è What is their goal? (e.g., Create and manage fillable forms): ")
    output_file = args.output_file

    run_intelligence_pipeline(pdf_dir, persona, job_to_be_done, output_file)